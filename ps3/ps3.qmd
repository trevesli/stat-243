---
title: "STAT 243 Problem Set 3"
author: "Treves Li"
date: now
format:
  pdf:
    code-block-border-left: false
output: true
---

## Collaboration Statement
I did not collaborate with anyone.


## Question 1
> Let’s investigate the structure of the `statsmodels` package to get some experience with the structure of a large Python package and with how `import` and the `__init__.py` file(s) are used. You’ll need to go into the `statsmodels` source code (see Unit 5). Also note that the following cases may involve functions, classes, and class methods. Be sure to be clear to say which of those you are talking about and if it’s a class, describe any inheritance structure.


## Question 1a
> For this subpart only, consider doing `import statsmodels`. What is in the `statsmodels` namespace that is created? Where (what module file) is the version number for `statsmodels` stored in? What is the absolute path to the package on the machine you are working on?
---

To access the `statsmodels` namespace, I use `dir()`:
```{python}
#| label: 1a-dir
#| cache: false
import statsmodels
dir(statsmodels)
```

It's interesting that the submodules normally associated with `statsmodels` (e.g., `statsmodels.api` or `statsmodels.tsa`) are not imported at the same time. Based on reading the `__init__.py`, it seems like the import is only loading the minimal high-level `statsmodels` module with some metadata-like dunders, perhaps in an effort to be space efficient. I can imagine that the entire package might use a lot of memory.

The version number for  `statsmodels` is stored in the `_version.py` file. I first tried looking for it in `statsmodels`'s `__init__.py`, but opening that file revealed that it imported the "version" information from another module/file based on this line: 

`from statsmodels._version import __version__, __version_tuple__`. 

We can verify by running the code chunk below:
```{python}
#| label: 1a-version
#| cache: true
print(statsmodels._version.__file__)
print(statsmodels.__version__)
```

To find the absolute path to the `statsmodels` package's source code, I utilise the `__file__` attribute:
```{python}
#| label: 1a-path
#| cache: true
print(statsmodels.__file__)
```


{{< pagebreak >}}


## Question 1b
> The remaining subparts all relate to using the standard `import statsmodels.api as sm` invocation. First, describe briefly what happens when this is run (what files are accessed). Then, describe what kind of object `MICE` is, how it is imported and where it is found. Do the same for `GLM`.
---

When I run the invocation, I can see what file is being run:
```{python}
#| label: 1b-file
#| cache: false
import statsmodels.api as sm
print(sm.__file__)
```

By looking closely at `api.py`, I can determine that the code is accessing each of the submodules to set up the namespace that it needs to operate. Specifically, it is importing the necessary functions, classes, submodules, and variables. All of this information on what is imported is made available to the user if they were to run `dir(sm)`.

### MICE
To find out what kind of object `MICE` is, how it is imported, and where it's found I run the following code:
```{python}
#| label: 1b-MICE-type
#| cache: true
print(type(sm.MICE))
```

The result shows that `MICE` is a class object.

By reading the `api.py` file, I can determine that it is directly imported using the following statement:
```{python}
#| label: 1b-MICE-import
#| cache: true
#| eval: false
from .imputation.mice import MICE
```

To find where the `MICE` module is, I use the information from the `import` statement while also performing `grep` on the `statsmodels` directory. I also run this code:
```{python}
#| label: 1b-MICE-module
#| cache: true
print(sm.MICE.__module__)
```
From all these information, I can find that `MICE`'s inheritance structure and location in:

`/home/treves/.local/lib/python3.10/site-packages/statsmodels/imputation/mice.py`

### GLM
I can do the same for the `GLM` module:

```{python}
#| label: 1b-GLM-type
#| cache: true
print(type(sm.GLM))
```

Again, `GLM` is a class object, which is directly imported through:

```{python}
#| label: 1b-GLM-import-1
#| cache: true
#| eval: false
from .genmod.api import GLM
```

If I open `/genmod/api.py`, I find that it is actually imported through the `generalized_linear_model` submodule:

```{python}
#| label: 1b-GLM-import-2
#| cache: true
#| eval: false
from .generalized_linear_model import GLM
```

which allows me to find its inheritance structure and true location here:

`~/.local/lib/python3.10/site-packages/statsmodels/genmod/generalized_linear_model.py`

I can verify the location by running this code chunk:
```{python}
#| label: 1b-GLM-module
#| cache: true
print(sm.GLM.__module__)
```


{{< pagebreak >}}


## Question 1c
> Consider `sm.gam`. What is in the namespace? Describe how the importing works and in what modules the objects in the namespace are defined.
---

To determine the namespace, I can use `dir()` on `sm.gam`:
```{python}
#| label: 1c-namespace
#| cache: true
dir(sm.gam)
```

To determine how it's imported, I can trace what file is actually being run.
```{python}
#| label: 1c-file
#| cache: true
print(sm.gam.__file__)
```

When I open `/gam/api.py`, I find that the importing works by directly accessing the objects in their respective submodules, in such a way that the objects (which are likely classes) now become directly available in the `gam` submodule's local namespace. The import statements are provided below as an example.
```{python}
#| label: 1c-import
#| eval: false
#| cache: true
from .generalized_additive_model import GLMGam
from .gam_cross_validation.gam_cross_validation import MultivariateGAMCVPath
from .smooth_basis import BSplines, CyclicCubicSplines
```

The submodules in which the objects are defined can be determined through the following code:
```{python}
#| label: 1c-submodules
#| cache: true

for i in sm.gam.__all__:
  object = getattr(sm.gam, i)
  print(f"'{i}' is in {object.__module__}")
```


{{< pagebreak >}}


## Question 1d
> Consider `sm.distributions.monotone_fn_inverter`. What is it, how it is imported and what file it is defined in?
---

To determine what type of object it is, I run:
```{python}
#| label: 1d-object
#| cache: true
print(type(sm.distributions.monotone_fn_inverter))
```
The output shows that it's a **function** inside the `sm.distributions` submodule. Since the object is a function, and not something like a class, then there is no explicit inheritance structure.

To determine how it is imported, I can `grep` the function name in the `statsmodels` directory, and find that it is called in the following file:

`~/.local/lib/python3.10/site-packages/statsmodels/distributions/__init__.py:`

Specifically, the code below from `__init__.py` shows that it is imported directly from the `empirical_distribution` submodule (which is itself from the `distributions` module) and into the `distributions` namespace.
```{python}
#| label: 1d-init
#| cache: true
#| eval: false
from .empirical_distribution import (
  ECDF, ECDFDiscrete, monotone_fn_inverter, StepFunction
  )
```

We confirm that the function is in the `distributions` namespace as a sanity check:
```{python}
#| label: 1d-dir
#| cache: true
import statsmodels.distributions 
'monotone_fn_inverter' in dir(sm.distributions)
```

Based on the information above, I can determine that the function is likely defined in some file called `empirical_distribution.py`. I can confirm this by running:
```{python}
#| label: 1d-file
#| cache: true
print(sm.distributions.empirical_distribution.__file__[-51:])
```

When I open the `empirical_distribution.py` file above, I indeed find that the function is defined there, specifically in Line 218:
```{python}
#| label: 1d-definition
#| cache: true
filename = sm.distributions.empirical_distribution.__file__

with open(filename, "r") as f:
  for line_num, line in enumerate(f, start=1):
    if "monotone_fn_inverter" in line:
      print(f"Line {line_num}: {line.strip()}")
```


{{< pagebreak >}}


## Question 2
> The website [Commission on Presidential Debates](https://debates.org/voter-education/debate-transcripts) has the text from recent debates between the candidates for President of the United States. (As a bit of background for those of you not familiar with the US political system, there are usually three debates between the Republican and Democratic candidates at which they are asked questions so that US voters can determine which candidate they would like to vote for.) Your task is to process the information and produce data on the debates. Note that while I present the problem below as subparts (a)-(d), your solution does not need to be divided into subparts in the same way, but you do need to make clear in your solution where and how you are doing what. For the purposes of this problem, please work on the the debates I’ve selected (see code below) for the years 2000, 2004, 2008, 2012, 2016, and 2020. (I’ve tried to select debates that cover domestic policy in whole or in part to control one source of variation, namely the topic of the debate.) I’ll call each individual response by a candidate to a question a “chunk”. A chunk might just be a few words or might be multiple paragraphs.
> 
> The goal of this problem is two-fold: first to give you practice with regular expressions and string processing and the second to have you thinking about writing well-structured, readable code (similar to question 4 of PS1). You can choose to use either a functional programming approach or an object-oriented approach. I strongly recommend that you use the approach that you are **less** familiar with so as to gain more experience. Please think about writing short, modular functions or methods. Explore the use of `map`, list comprehension or other techniques to avoid having a lot of nested for loops. Think carefully about how to structure your objects to store the spoken chunks so that the structure works well with your functions/methods. Note that for this problem, for the sake of time, you do not need extensive docstrings, but it should still be clear what each function does. In parts (a)-(c), add simple sanity checks that you are getting reasonable results.
>
> Given that in earlier problem sets, you already worked on downloading and processing HTML, I’m giving you the code (in the file `ps/ps3prob3.py` in the class repository) to download the HTML and do some initial processing, so you can dive right into processing the actual debate text.
---

I have more experience with using functional programming, so I attempted this question using object-oriented programming.

First, I execute the Python file to download and process the HTML. 
```{python}
#| label: 2-extract-py
#| cache: true
#| output: false
ps3prob3 = r"~/fall-2024/ps/ps3prob3.py"

# Expand to full path
import os.path

ps3prob3 = os.path.expanduser(ps3prob3)

with open(ps3prob3) as f:
  script = f.read()

exec(script)

# The code chunk is amended so as not to generate output
# This is to make Quarto formatting neater for this assignment
```

Each debate transcript is thus saved as an element in the list `debates_body`.

I also use the opportunity to extract the list of `candidates` from the `ps3prob3.py` file, as well as the list of `moderators`. 

```{python}
#| label: get-names
#| cache: true

# Get the names of debaters for years of interest 
candidates = [person for entry in candidates for person in entry.values()]

# Retrieve only unique names, for both candidates and moderators
candidates = list(set(candidates))
moderators = list(set(moderators))

# Combine to make a list of speakers
speakers = candidates + moderators

print(candidates)
print(moderators)
```


{{< pagebreak >}}


## Question 2a
> Convert the text so that for each debate, the spoken text is split up into individual chunks of text spoken by each speaker (including the moderator). If there are two chunks in a row spoken by a candidate, combine them into a single chunk. Make sure that any formatting and non-spoken text (e.g., the tags for ‘Laughter’ and ‘Applause’) is stripped out. Report the number of chunks per speaker.

I first create a class called `Debate`, and define its methods. 

In `find_non_spoken`, I find and strip out non-spoken text, which is normally indicated by single words being in parentheses or square brackets. In `strip_non_spoken`, I trip the transcript of the non-spoken texts. I wrote some additional functions for sanity checks.

```{python}
import re

class Debate:

  def __init__(self, debates_body):
      self.debates_body = debates_body

  def find_non_spoken(self):
    """
    Searches and extracts non-spoken text (e.g., "Laughter" or "Applause") in
    parentheses or square brackets from debates.

    Returns:
        list: A list of non-spoken words found in the debates.
    """

    # Regex to find all single words that appear within parentheses
    # and square brackets
    non_spoken_pattern = r"[\(\[]\w+[\)\]]"
    non_spoken_matches = [
        re.findall(non_spoken_pattern, debate) for debate in self.debates_body
    ]

    # Flatten the list of lists using list comprehension
    non_spoken_matches = [
        match for debate in non_spoken_matches for match in debate
    ]

    return non_spoken_matches

  def strip_non_spoken(self):
    """
    Strips non-spoken text from debates using regex.

    Returns:
        list: A list of debates with non-spoken text removed.
    """

    # Create new regex pattern based on non_spoken_matches
    non_spoken_list_pattern = "|".join(map(re.escape, self.find_non_spoken()))

    # Strip out all non-spoken text using regex
    # Strip out leading and trailing whitespaces
    debates_stripped = [
        re.sub(non_spoken_list_pattern, "", debate).strip()
        for debate in self.debates_body
    ]

    return debates_stripped

  def count_non_spoken(self, non_spoken_word):
    """
    Count occurrences of a specified "non-spoken word" in a debate.

    Returns:
        list: Counts of occurrences of the word in each debate.
    """
    counts = [debate.lower().count(non_spoken_word) for debate in self.debates_body]
    return counts

  def compare_pre_post_strip(self, non_spoken_word):
    """
    Compare the counts of a specified non-spoken word before and after
    stripping "non-spoken" text.

    Returns:
        None: Prints counts before and after stripping.
    """
    counts_pre_strip = self.count_non_spoken(non_spoken_word)
    stripped_debates = Debate(self.strip_non_spoken())
    counts_post_strip = stripped_debates.count_non_spoken(non_spoken_word)
    return print(f"Counts of '{non_spoken_word}': \n\
          Pre-strip:\t {counts_pre_strip} \n\
          Post-strip:\t {counts_post_strip}\n")
```

I test the class and its methods below, while also running some sanity checks:
```{python}
#| label: 2a-debates-sanity
#| cache: true

# Instantiate `Debate` class
debates = Debate(debates_body)

# Strip debates of non-spoken words
debates_stripped = debates.strip_non_spoken()

# Check that non-spoken words are properly stripped
debates.compare_pre_post_strip("applause")
debates.compare_pre_post_strip("laughter")
debates.compare_pre_post_strip("crosstalk")
```

To retrieve the chunks, I create a new class called `Chunk` that takes a string/transcript. In `get_debate_year`, I extract the year corresponding to each debate. I wrote `strip_preamble` to get rid of the preamble in each transcript, which would otherwise interfere when counting the chunks of each speaker. 

In `get_chunks`, I subdivide each transcript into chunks based on the idea that each chunk will be marked by a speaker (candidate or moderator; stylised in all caps) followed by a colon, using regex. I needed to adjust the regex iteratively, since each transcript had its own transcription idiosyncracies.

In `count_chunks`, I count the number of occurrences that each speaker has their own chunk in a debate, ensuring that only speakers that appear in the `speakers` list are included in the resultant dictionary.

```{python}
#| label: 2a-chunk
#| cache: true
class Chunk:
  def __init__(self, transcript):
    self.transcript = transcript

  def get_debate_year(self):
    """
    Gets the debate year.

    Returns:
        int: The year of the debate
    """
    year_pattern = r"^.+?(\d{4})"
    year_matches = re.findall(year_pattern, self.transcript)
    matches_list = [int(match) for match in year_matches]

    return matches_list[0]

  def strip_preamble(self):
    """
    Strips the transcript preamble up to the first time a name in `moderator` 
    is encountered.

    Returns:
        str: Transcript with preamble removed.
    """
    preamble_pattern = r"^(.*?)(?:" \
                      + "|".join(re.escape(moderator) for moderator in moderators) \
                      + r"):\s*(.*)"
    preamble_match = re.search(preamble_pattern, self.transcript)
    self.transcript = self.transcript[preamble_match.end(1):].strip()
    return self.transcript

  def get_chunks(self):
    """
    Isolates and consolidates chunks of dialogue from the transcript 
    using regex.

    Returns:
        list: A list of lists, where inner list contains the speaker's 
              identifier and their consolidated speech chunk.
    """
    # Use regex to find all chunks
    chunk_pattern = r"([A-Z]+): (?<=[A-Z]: )(.*?)(?=[A-Z]*:|$)"
    chunk_matches = re.findall(chunk_pattern, self.transcript)
    matches_list = [list(match) for match in chunk_matches]

    # Consolidate chunks if spoken in a row by the same speaker
    for i in range(len(matches_list) - 1):
      if matches_list[i][0] == matches_list[i+1][0]:
        matches_list[i][1] += matches_list[i+1][1]
        matches_list[i+1][0] = None
        matches_list[i+1][1] = None
    
    # Drop rows where the values are "None"
    filtered_chunks = [chunk for chunk in matches_list 
                      if chunk[0] is not None 
                      and chunk[1] is not None] 
    
    return filtered_chunks

  def count_chunks(self, chunks):
    """
    Counts the number of chunks per speaker for each debate.
    Only speakers in the `speakers` list are considered.

    Returns:
        dict: A dict where the keys are speaker names, and the values 
              are the counts of chunks per speaker.
    """
    speaker_count = {}

    for chunk in chunks:
      speaker = chunk[0]  # The first element is the speaker

      # Ensure speaker is only counted if they are in `speakers` list
      if speaker in speaker_count and speaker in speakers:
        # Increment counter for a particular speaker
        speaker_count[speaker] += 1  
      elif speaker in speakers:
        # Initialise value for first time speaker is encoutered
        speaker_count[speaker] = 1

    return speaker_count

  def check_chunk_consol(self):
    ...
    #See if chunk and chunk+1 speakers are the same
    #TODO
```

Again, I test the class and run some sanity checks:
```{python}
#| label: 2a-chunk-sanity
#| cache: true

deb2020 = Chunk(transcript[6])
print(debates_stripped[5])

with open('debate2020_pre.txt', 'w', newline = '') as txtfile:
    my_writer = txtfile.write(debates_body[5])
with open('debate2020.txt', 'w', newline = '') as txtfile:
    my_writer = txtfile.write(debates_stripped[5])

#print(deb1994.strip_preamble())

# Assign each debate to a separate list of lists
for transcript in debates_stripped:
  debate = Chunk(transcript)
  year = debate.get_debate_year()
  
  debate.strip_preamble()
  
  debate_chunked = debate.get_chunks()
  print(debate_chunked)
  chunk_counts = debate.count_chunks(debate_chunked)
  print(f"Chunk counts for {year}: {chunk_counts}")
```



Note that the single occurrence of "applause" from the post-stripped transcript was from the moderator literally saying the word "applause". 

