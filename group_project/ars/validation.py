import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt

def is_log_concave(f, x_range):
    """
    Check if the function f is log-concave over the x_range.
    
    Args:
        f(function): Probability density function (maybe unnormalized)
        x_range(array_like): Range of x values to check
    
    Returns: 
        bool: Whether the function f is concave or not.
    """
    x = np.asarray(x_range)
    f_values = f(x)
    
    if np.any(f_values <= 0):
        raise ValueError("Function values must be positive.")
    
    log_f_values = np.log(f_values)
    
    if log_f_values.ndim == 0:  # Scalar check
        raise ValueError("Function should return an array of values, not a scalar.")
    
    # Compute second derivative
    log_f_second_derivative = np.gradient(np.gradient(log_f_values, x), x)

    # Check second derivative is non-positive (i.e. log-concave condition)
    if np.any(log_f_second_derivative > 1e-10):
        raise ValueError("The function is not log-concave!")    
    is_log_concave = np.all(log_f_second_derivative <= 1e-10)
    
    return is_log_concave

def compare_samples_to_distribution(samples, target_pdf, domain, check_mean=True):
    """
    Compare ARS-generated samples to the target distribution.

    Args:
        samples (np.ndarray): Samples generated by ARS.
        target_pdf (function): The target probability density function.
        domain (tuple): Domain of the distribution as (min, max).
        check_mean (bool): Whether to include mean comparison in the hierarchy.

    Returns:
        None. (Plots and/or prints comparison results.)
    """
    if len(samples) == 0:
        print("KS Test skipped, sample array is empty.")
        return

    try:
        # 1. If calculable, compare mean
        if check_mean:
            theoretical_mean, success = integrate_mean(target_pdf, domain)
            if success:
                sample_mean = np.mean(samples)
                print(f"Theoretical Mean: {theoretical_mean:.4f}")
                print(f"Sample Mean: {sample_mean:.4f}")
                print(f"Difference in Means: {np.abs(theoretical_mean - sample_mean):.4f}")
                # plt.figure()
                # plt.bar(["Theoretical Mean", "Sample Mean"], [theoretical_mean, sample_mean], color=["red", "blue"])
                # plt.title("Comparison of Means")
                # plt.show()
                # return  # Skip others

    except Exception as e:
        print(f"Mean comparison skipped: {e}")

    try:
        # 2. KS Test
        print("Performing KS Test...")
        empirical_cdf = lambda x: np.mean(samples <= x)
        ks_stat, ks_pvalue = stats.kstest(samples, empirical_cdf)
        print(f"KS Statistic: {ks_stat:.4f}, p-value: {ks_pvalue:.4f}")

        if ks_pvalue < 0.05:
            print("Warning: KS Test suggests samples differ significantly from target distribution.")
        else:
            print("KS Test suggests samples align well with target distribution.")

    except Exception as e:
        print(f"KS Test skipped: {e}")

    try:
        # 3. Quantile-Quantile (QQ) Plot
        print("Generating QQ Plot...")
        theoretical_quantiles = np.linspace(domain[0], domain[1], len(samples))
        sample_quantiles = np.quantile(samples, np.linspace(0, 1, len(samples)))

        plt.figure()
        plt.scatter(theoretical_quantiles, sample_quantiles, label="Empirical vs Theoretical")
        plt.plot(theoretical_quantiles, theoretical_quantiles, color="red", linestyle="--", label="Ideal Fit")
        plt.title("QQ Plot")
        plt.xlabel("Theoretical Quantiles")
        plt.ylabel("Sample Quantiles")
        plt.legend()
        plt.show()

    except Exception as e:
        print(f"QQ Plot skipped: {e}")

    try:
        # 4. KL Divergence
        print("Calculating KL Divergence...")
        sample_pdf_values = target_pdf(samples)
        sample_pdf_values = np.maximum(sample_pdf_values, 1e-10)  # Avoid division by zero
        kl_div = np.mean(np.log(sample_pdf_values) - np.log(stats.gaussian_kde(samples)(samples)))
        print(f"KL Divergence: {kl_div:.4f}")

    except Exception as e:
        print(f"KL Divergence skipped: {e}")

def integrate_mean(pdf, domain, num_points=1000):
    """
    Numerically integrate the mean of a target PDF.

    Args:
        pdf (function): Target probability density function.
        domain (tuple): Domain of the distribution as (min, max).
        num_points (int): Number of points for numerical integration.

    Returns:
        (mean, success) (float, bool): Theoretical mean and success status.
    """
    try:
        x_vals = np.linspace(*domain, num_points)
        pdf_values = pdf(x_vals)
        pdf_values = np.maximum(pdf_values, 1e-10)  # Avoid numerical issues
        mean = np.trapezoid(x_vals * pdf_values, x_vals) / np.trapezoid(pdf_values, x_vals)
        return mean, True
    except Exception as e:
        print(f"Error integrating mean: {e}")
        return None, False

